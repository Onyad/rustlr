//Parser generated by rustlr for grammar test1

#![allow(unused_variables)]
#![allow(non_snake_case)]
#![allow(non_camel_case_types)]
#![allow(unused_parens)]
#![allow(unused_mut)]
#![allow(unused_imports)]
#![allow(unused_assignments)]
#![allow(dead_code)]
#![allow(irrefutable_let_patterns)]
#![allow(unreachable_patterns)]
extern crate rustlr;
use rustlr::{Tokenizer,TerminalToken,ZCParser,ZCRProduction,Stateaction,decode_action};
use rustlr::{StrTokenizer,RawToken,LexSource};
use std::collections::{HashMap,HashSet};

const SYMBOLS:[&'static str;10] = ["E","T","F","+","*","(",")","num","START","EOF"];

const TABLE:[u64;45] = [8590131201,4295229441,65537,21475164160,30064902144,281487862005760,281513631416323,562975723552770,562962838650882,562967133618178,562988608454658,844442110197762,844437815230466,844463585034242,844450700132354,1125938561613826,1125925676711938,1125912791810050,1125917087170560,1407379178782721,1407396358717440,1407374884077569,1407404948455424,1407383473684481,1688879925166080,1688871335428096,1688858450395137,1688854155821057,1970346312138752,1970333427564545,1970354901876736,2251812698980352,2251825584209920,2533313445101570,2533300560199682,2533287675297794,2533291970723840,2814762652139522,2814775537041410,2814766947106818,2814788421943298,3096237628981250,3096250513883138,3096263398785026,3096241923948546,];

pub fn make_parser() -> ZCParser<i32,i32>
{
 let mut parser1:ZCParser<i32,i32> = ZCParser::new(7,12);
 let mut rule = ZCRProduction::<i32,i32>::new_skeleton("start");
 rule = ZCRProduction::<i32,i32>::new_skeleton("E");
 rule.Ruleaction = |parser|{ let mut t = parser.popstack(); let mut _item1_ = parser.popstack(); let mut e = parser.popstack();  e.value + t.value };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<i32,i32>::new_skeleton("E");
 rule.Ruleaction = |parser|{ let mut t = parser.popstack();  t.value };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<i32,i32>::new_skeleton("T");
 rule.Ruleaction = |parser|{ let mut _item2_ = parser.popstack(); let mut _item1_ = parser.popstack(); let mut _item0_ = parser.popstack(); 
  if let ((f),(t),)=(_item2_.value,_item0_.value,) {  t*f }  else {parser.bad_pattern("((f),(t),)")} };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<i32,i32>::new_skeleton("T");
 rule.Ruleaction = |parser|{ let mut _item0_ = parser.popstack(); 
  if let ((f),)=(_item0_.value,) {  f }  else {parser.bad_pattern("((f),)")} };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<i32,i32>::new_skeleton("F");
 rule.Ruleaction = |parser|{ let mut _item2_ = parser.popstack(); let mut e = parser.popstack(); let mut _item0_ = parser.popstack();  e.value };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<i32,i32>::new_skeleton("F");
 rule.Ruleaction = |parser|{ let mut n = parser.popstack();  n.value };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<i32,i32>::new_skeleton("START");
 rule.Ruleaction = |parser|{ let mut _item0_ = parser.popstack(); <i32>::default()};
 parser1.Rules.push(rule);
 parser1.Errsym = "";

 for i in 0..45 {
   let symi = ((TABLE[i] & 0x0000ffff00000000) >> 32) as usize;
   let sti = ((TABLE[i] & 0xffff000000000000) >> 48) as usize;
   parser1.RSM[sti].insert(SYMBOLS[symi],decode_action(TABLE[i]));
 }

 for s in SYMBOLS { parser1.Symset.insert(s); }

 load_extras(&mut parser1);
 return parser1;
} //make_parser


// Lexical Scanner using RawToken and StrTokenizer
pub struct test1lexer<'t> {
   stk: StrTokenizer<'t>,
   keywords: HashSet<&'static str>,
}
impl<'t> test1lexer<'t> 
{
  pub fn from_str(s:&'t str) -> test1lexer<'t>  {
    Self::new(StrTokenizer::from_str(s))
  }
  pub fn from_source(s:&'t LexSource<'t>) -> test1lexer<'t>  {
    Self::new(StrTokenizer::from_source(s))
  }
  pub fn new(mut stk:StrTokenizer<'t>) -> test1lexer<'t> {
    let mut keywords = HashSet::with_capacity(16);
    for kw in [] {keywords.insert(kw);}
    for c in ['+','*','(',')',] {stk.add_single(c);}
    for d in [] {stk.add_double(d);}
    test1lexer {stk,keywords}
  }
}
impl<'t> Tokenizer<'t,i32> for test1lexer<'t>
{
   fn nextsym(&mut self) -> Option<TerminalToken<'t,i32>> {
    let tokopt = self.stk.next_token();
    if let None = tokopt {return None;}
    let token = tokopt.unwrap();
    match token.0 {
      RawToken::Alphanum(sym) if self.keywords.contains(sym) => Some(TerminalToken::from_raw(token,sym,<i32>::default())),
      RawToken::Num(n) => Some(TerminalToken::from_raw(token,"num",(n as i32))),
      RawToken::Symbol(s) => Some(TerminalToken::from_raw(token,s,<i32>::default())),
      RawToken::Alphanum(s) => Some(TerminalToken::from_raw(token,s,<i32>::default())),
      _ => Some(TerminalToken::from_raw(token,"<LexicalError>",<i32>::default())),
    }
  }
   fn linenum(&self) -> usize {self.stk.line()}
   fn column(&self) -> usize {self.stk.column()}
   fn position(&self) -> usize {self.stk.current_position()}
}//impl Tokenizer

fn load_extras(parser:&mut ZCParser<i32,i32>)
{
}//end of load_extras: don't change this line as it affects augmentation
