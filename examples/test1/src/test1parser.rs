//Parser generated by rustlr for grammar test1

#![allow(unused_variables)]
#![allow(non_snake_case)]
#![allow(non_camel_case_types)]
#![allow(unused_parens)]
#![allow(unused_mut)]
#![allow(unused_imports)]
#![allow(unused_assignments)]
#![allow(dead_code)]
#![allow(irrefutable_let_patterns)]
#![allow(unreachable_patterns)]
extern crate rustlr;
use rustlr::{Tokenizer,TerminalToken,ZCParser,ZCRProduction,Stateaction,decode_action};
use rustlr::{StrTokenizer,RawToken,LexSource};
use std::collections::{HashMap,HashSet};

static SYMBOLS:[&'static str;10] = ["E","T","F","+","*","(",")","num","START","EOF"];

static TABLE:[u64;45] = [4295032833,21474967552,262145,8590262273,30064967680,281492156973056,281513631481858,281487861678082,281500746579970,562949953880065,562971428388864,562980018388992,562958543683585,562954248454145,844463585165314,844437815361538,844442110328834,844450700263426,1125912792268800,1125938561548291,1407387768651778,1407400653553666,1407392063619074,1407413538455554,1688858450788353,1688871335231488,1688879925231616,1970350607433728,1970337722400768,2251829878652928,2251804109373441,2251821288652800,2251808403947521,2533291970396162,2533300560330754,2533313445232642,2533287675428866,2814766947237890,2814762652270594,2814788422074370,2814775537172482,3096250513620994,3096241924079616,3096237628719106,3096263398522882,];

pub fn make_parser() -> ZCParser<i32,i32>
{
 let mut parser1:ZCParser<i32,i32> = ZCParser::new(7,12);
 let mut rule = ZCRProduction::<i32,i32>::new_skeleton("start");
 rule = ZCRProduction::<i32,i32>::new_skeleton("E");
 rule.Ruleaction = |parser|{ let mut t = parser.popstack(); let mut _item1_ = parser.popstack(); let mut e = parser.popstack();  e.value + t.value };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<i32,i32>::new_skeleton("E");
 rule.Ruleaction = |parser|{ let mut t = parser.popstack();  t.value };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<i32,i32>::new_skeleton("T");
 rule.Ruleaction = |parser|{ let mut _item2_ = parser.popstack(); let mut _item1_ = parser.popstack(); let mut _item0_ = parser.popstack(); 
  if let ((f),(t),)=(_item2_.value,_item0_.value,) {  t*f }  else {parser.bad_pattern("((f),(t),)")} };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<i32,i32>::new_skeleton("T");
 rule.Ruleaction = |parser|{ let mut _item0_ = parser.popstack(); 
  if let ((f),)=(_item0_.value,) {  f }  else {parser.bad_pattern("((f),)")} };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<i32,i32>::new_skeleton("F");
 rule.Ruleaction = |parser|{ let mut _item2_ = parser.popstack(); let mut e = parser.popstack(); let mut _item0_ = parser.popstack();  e.value };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<i32,i32>::new_skeleton("F");
 rule.Ruleaction = |parser|{ let mut n = parser.popstack();  n.value };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<i32,i32>::new_skeleton("START");
 rule.Ruleaction = |parser|{ let mut _item0_ = parser.popstack(); <i32>::default()};
 parser1.Rules.push(rule);
 parser1.Errsym = "";

 for i in 0..45 {
   let symi = ((TABLE[i] & 0x0000ffff00000000) >> 32) as usize;
   let sti = ((TABLE[i] & 0xffff000000000000) >> 48) as usize;
   parser1.RSM[sti].insert(SYMBOLS[symi],decode_action(TABLE[i]));
 }

 for s in SYMBOLS { parser1.Symset.insert(s); }

 load_extras(&mut parser1);
 return parser1;
} //make_parser


// Lexical Scanner using RawToken and StrTokenizer
pub struct test1lexer<'t> {
   stk: StrTokenizer<'t>,
   keywords: HashSet<&'static str>,
}
impl<'t> test1lexer<'t> 
{
  pub fn from_str(s:&'t str) -> test1lexer<'t>  {
    Self::new(StrTokenizer::from_str(s))
  }
  pub fn from_source(s:&'t LexSource<'t>) -> test1lexer<'t>  {
    Self::new(StrTokenizer::from_source(s))
  }
  pub fn new(mut stk:StrTokenizer<'t>) -> test1lexer<'t> {
    let mut keywords = HashSet::with_capacity(16);
    for kw in [] {keywords.insert(kw);}
    for c in ['+','*','(',')',] {stk.add_single(c);}
    for d in [] {stk.add_double(d);}
    test1lexer {stk,keywords}
  }
}
impl<'t> Tokenizer<'t,i32> for test1lexer<'t>
{
   fn nextsym(&mut self) -> Option<TerminalToken<'t,i32>> {
    let tokopt = self.stk.next_token();
    if let None = tokopt {return None;}
    let token = tokopt.unwrap();
    match token.0 {
      RawToken::Num(n) => Some(TerminalToken::from_raw(token,"num",(n as i32))),
      RawToken::Symbol(s) => Some(TerminalToken::from_raw(token,s,<i32>::default())),
      RawToken::Alphanum(s) => Some(TerminalToken::from_raw(token,s,<i32>::default())),
      _ => Some(TerminalToken::from_raw(token,"<LexicalError>",<i32>::default())),
    }
  }
   fn linenum(&self) -> usize {self.stk.line()}
   fn column(&self) -> usize {self.stk.column()}
   fn position(&self) -> usize {self.stk.current_position()}
}//impl Tokenizer

fn load_extras(parser:&mut ZCParser<i32,i32>)
{
}//end of load_extras: don't change this line as it affects augmentation
