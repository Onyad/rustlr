//Parser generated by rustlr for grammar nuttycalc

#![allow(unused_variables)]
#![allow(non_snake_case)]
#![allow(non_camel_case_types)]
#![allow(unused_parens)]
#![allow(unused_mut)]
#![allow(unused_imports)]
#![allow(unused_assignments)]
#![allow(dead_code)]
#![allow(irrefutable_let_patterns)]
#![allow(unreachable_patterns)]
use std::rc::Rc;
use std::cell::RefCell;
extern crate rustlr;
use rustlr::{Tokenizer,TerminalToken,ZCParser,ZCRProduction,Stateaction,decode_action};
use rustlr::{StrTokenizer,RawToken,LexSource};
use std::collections::{HashMap,HashSet};

static SYMBOLS:[&'static str;17] = ["_WILDCARD_TOKEN_","E","E2","E3","num","float","(",")","mark1","mark2","PLUS","MINUS","TIMES","DIV","START","EOF","NEWDELAYNT_E_4"];

static TABLE:[u64;58] = [25770000384,17180000256,4295032833,281539401220099,563014377930754,844493650329601,844442110656512,844450700722176,844429225361409,1125917087236096,1125929972072448,1970389261680642,2251851357880320,2814766948679680,2814779831877634,3096276284276736,3377725492690944,3377768442298369,3377704016543745,3377716902821888,3659204762927104,3940714099048450,4503629696270336,4503616811302912,7318405230821376,7318400934608898,7599854437793792,7881363772538882,9007229321674752,9288691413549056,9288704296353794,9570200748163074,9851675728412672,10133116343877632,10133124933746688,10133167883747329,10133103457599489,10414591320653824,10414604203065346,10696104951676928,10696100655136770,11540525587890176,13510828950093824,13792303924183042,13792291041968128,14073800375795714,15199700282441730,15762650235731970,16888528671408128,17169990762496000,17451500099862528,17732953597739010,17732940716179456,18014428578512896,18295903551029250,18295890669731840,19703278439170048,19984787771031554,];

pub fn make_parser() -> ZCParser<(),()>
{
 let mut parser1:ZCParser<(),()> = ZCParser::new(11,77);
 let mut rule = ZCRProduction::<(),()>::new_skeleton("start");
 rule = ZCRProduction::<(),()>::new_skeleton("E");
 rule.Ruleaction = |parser|{ let mut _item0_ = parser.popstack(); <()>::default()};
 parser1.Rules.push(rule);
 rule = ZCRProduction::<(),()>::new_skeleton("E");
 rule.Ruleaction = |parser|{ let mut _item4_ = parser.popstack(); let mut _item3_ = parser.popstack(); let mut _item2_ = parser.popstack(); let mut _item1_ = parser.popstack(); let mut _item0_ = parser.popstack(); <()>::default()};
 parser1.Rules.push(rule);
 rule = ZCRProduction::<(),()>::new_skeleton("E");
 rule.Ruleaction = |parser|{ let mut _item4_ = parser.popstack(); let mut _item3_ = parser.popstack(); let mut _item2_ = parser.popstack(); let mut _item1_ = parser.popstack(); let mut _item0_ = parser.popstack(); <()>::default()};
 parser1.Rules.push(rule);
 rule = ZCRProduction::<(),()>::new_skeleton("E");
 rule.Ruleaction = |parser|{ let mut _item2_ = parser.popstack(); let mut _item1_ = parser.popstack(); let mut _item0_ = parser.popstack(); <()>::default()};
 parser1.Rules.push(rule);
 rule = ZCRProduction::<(),()>::new_skeleton("START");
 rule.Ruleaction = |parser|{ let mut _item4_ = parser.popstack(); let mut _item3_ = parser.popstack(); let mut _item2_ = parser.popstack(); let mut _item1_ = parser.popstack(); let mut _item0_ = parser.popstack(); <()>::default()};
 parser1.Rules.push(rule);
 rule = ZCRProduction::<(),()>::new_skeleton("NEWDELAYNT_E_4");
 rule.Ruleaction = |parser|{ let mut _item_del5_1_ = parser.popstack(); let mut _item4_ = parser.popstack(); let mut _item3_ = parser.popstack(); let mut _item2_ = parser.popstack(); let mut _item1_ = parser.popstack(); let mut _item0_ = parser.popstack();  let _delvar_16_1_ = { }; (_delvar_16_1_,_item_del5_1_,) };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<(),()>::new_skeleton("NEWDELAYNT_E_4");
 rule.Ruleaction = |parser|{ let mut _item_del5_2_ = parser.popstack(); let mut _item4_ = parser.popstack(); let mut _item3_ = parser.popstack(); let mut _item2_ = parser.popstack(); let mut _item1_ = parser.popstack(); let mut _item0_ = parser.popstack();  let _delvar_16_1_ = { }; (_delvar_16_1_,_item_del5_2_,) };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<(),()>::new_skeleton("NEWDELAYNT_E_4");
 rule.Ruleaction = |parser|{ let mut _item_del3_3_ = parser.popstack(); let mut _item2_ = parser.popstack(); let mut _item1_ = parser.popstack(); let mut _item0_ = parser.popstack();  let _delvar_16_1_ = { }; (_delvar_16_1_,_item_del3_3_,) };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<(),()>::new_skeleton("NEWDELAYNT_E_4");
 rule.Ruleaction = |parser|{ let mut _item_del1_0_ = parser.popstack(); let mut _item0_ = parser.popstack();  let _delvar_16_1_ = { }; (_delvar_16_1_,_item_del1_0_,) };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<(),()>::new_skeleton("E");
 rule.Ruleaction = |parser|{ let mut _item3_ = parser.popstack(); let mut _item2_ = parser.popstack(); let mut _delayeditem1_ = parser.popstack(); let mut _item0_ = parser.popstack();  let mut _item1_ = _delayeditem1_.0; let mut _item2_ = _delayeditem1_.1; };
 parser1.Rules.push(rule);
 rule = ZCRProduction::<(),()>::new_skeleton("NEWDELAYNT_E_4");
 rule.Ruleaction = |parser|{ let mut _item_del5_1_ = parser.popstack(); let mut _item3_ = parser.popstack(); let mut _item2_ = parser.popstack(); let mut _delayeditem1_ = parser.popstack(); let mut _item0_ = parser.popstack();  let mut _item1_ = _delayeditem1_.0; let mut _item2_ = _delayeditem1_.1;  let _delvar_16_1_ = { }; (_delvar_16_1_,_item_del5_1_,) };
 parser1.Rules.push(rule);
 parser1.Errsym = "";

 for i in 0..58 {
   let symi = ((TABLE[i] & 0x0000ffff00000000) >> 32) as usize;
   let sti = ((TABLE[i] & 0xffff000000000000) >> 48) as usize;
   parser1.RSM[sti].insert(SYMBOLS[symi],decode_action(TABLE[i]));
 }

 for s in SYMBOLS { parser1.Symset.insert(s); }

 load_extras(&mut parser1);
 return parser1;
} //make_parser

pub fn parse_with<'t>(parser:&mut ZCParser<(),()>, lexer:&mut nuttycalclexer<'t>) -> Result<(),()>
{
  let _xres_ = parser.parse(lexer);  if !parser.error_occurred() {Ok(_xres_)} else {Err(_xres_)}
}//parse_with public function

pub fn parse_train_with<'t>(parser:&mut ZCParser<(),()>, lexer:&mut nuttycalclexer<'t>, parserpath:&str) -> Result<(),()>
{
  let _xres_ = parser.parse_train(lexer,parserpath);  if !parser.error_occurred() {Ok(_xres_)} else {Err(_xres_)}
}//parse_train_with public function

// Lexical Scanner using RawToken and StrTokenizer
pub struct nuttycalclexer<'t> {
   stk: StrTokenizer<'t>,
   keywords: HashSet<&'static str>,
   lexnames: HashMap<&'static str,&'static str>,
   shared_state: Rc<RefCell<()>>,
}
impl<'t> nuttycalclexer<'t> 
{
  pub fn from_str(s:&'t str) -> nuttycalclexer<'t>  {
    Self::new(StrTokenizer::from_str(s))
  }
  pub fn from_source(s:&'t LexSource<'t>) -> nuttycalclexer<'t>  {
    Self::new(StrTokenizer::from_source(s))
  }
  pub fn new(mut stk:StrTokenizer<'t>) -> nuttycalclexer<'t> {
    let mut lexnames = HashMap::with_capacity(64);
    let mut keywords = HashSet::with_capacity(64);
    let shared_state = Rc::new(RefCell::new(<()>::default()));
    for kw in ["mark1","mark2","float","_WILDCARD_TOKEN_","num",] {keywords.insert(kw);}
    for c in ['(',')','/','+','-','*',] {stk.add_single(c);}
    for d in [] {stk.add_double(d);}
    for d in [] {stk.add_triple(d);}
    for (k,v) in [(r"/","DIV"),(r"+","PLUS"),(r"-","MINUS"),(r"*","TIMES"),] {lexnames.insert(k,v);}
    nuttycalclexer {stk,keywords,lexnames,shared_state}
  }
}
impl<'t> Tokenizer<'t,()> for nuttycalclexer<'t>
{
   fn nextsym(&mut self) -> Option<TerminalToken<'t,()>> {
    let tokopt = self.stk.next_token();
    if let None = tokopt {return None;}
    let token = tokopt.unwrap();
    match token.0 {
      RawToken::Alphanum(sym) if self.keywords.contains(sym) => {
        let truesym = self.lexnames.get(sym).unwrap_or(&sym);
        Some(TerminalToken::from_raw(token,truesym,<()>::default()))
      },
      RawToken::Symbol(s) if self.lexnames.contains_key(s) => {
        let tname = self.lexnames.get(s).unwrap();
        Some(TerminalToken::from_raw(token,tname,<()>::default()))
      },
      RawToken::Symbol(s) => Some(TerminalToken::from_raw(token,s,<()>::default())),
      RawToken::Alphanum(s) => Some(TerminalToken::from_raw(token,s,<()>::default())),
      _ => Some(TerminalToken::from_raw(token,"<LexicalError>",<()>::default())),
    }
  }
   fn linenum(&self) -> usize {self.stk.line()}
   fn column(&self) -> usize {self.stk.column()}
   fn position(&self) -> usize {self.stk.current_position()}
   fn current_line(&self) -> &str {self.stk.current_line()}
   fn get_line(&self,i:usize) -> Option<&str> {self.stk.get_line(i)}
   fn get_slice(&self,s:usize,l:usize) -> &str {self.stk.get_slice(s,l)}
}//impl Tokenizer

fn load_extras(parser:&mut ZCParser<(),()>)
{
}//end of load_extras: don't change this line as it affects augmentation
